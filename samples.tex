% !TEX root = sputnik.tex
\documentclass[sputnik.tex]{subfiles}
\begin{document}

\def\Fr{\mathop{\rm Fr}}

\def\vc{{\footnotesize VC}}
\def\nip{{\footnotesize NIP}}


\def\medrel#1{\parbox[t]{6ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{25ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}



\chapter{Samples and approximations of measures}
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%


\section{Samples and subsamples}\label{multisets}

\def\ceq#1#2#3{\parbox[t]{32ex}{$\displaystyle #1$}\parbox{5ex}{$\displaystyle\hfil #2$}{$\displaystyle #3$}}

A sample is a sequence of elements of $\U$ where we disregard the order and only consider the number of times they appear.
Formally, a \emph{sample\/} is a \emph{multi-subset of $\U$}, that is is a function $A:\U\to\NN$. 

We interpret $A(x)$ as the \emph{multiplicity\/} of the element $x\in\U$.
The \emph{support\/} of $A$ is the set \emph{$\supp(A)$}$=\{x:A(x)\neq0\}$.
The \emph{size of $A$\/} is defined as

\ceq{\hfill\emph{$|A|$}}{=}{\sum_{x\in\U} A(x).}

If we identify sets with $\{0,1\}$-valued samples, the size generalizes cardinality. 

\makeatletter
\def\dotminus{\mathbin{\ooalign{\hidewidth\raise1ex\hbox{.}\hidewidth\cr$\m@th-$\cr}}}
\makeatother

We say that $C$ is a \emph{subsample\/} of $A$, and write \emph{$C\subseteq A$}, if $C(x)\le A(x)$ for all $x\in\U$.
We also define the \emph{intersection\/} and the \emph{difference\/} of two samples.
The element $x\in\U$ has multiplicity  $A(x)\wedge C(x)$ in \emph{$A\cap C$\/} and multiplicity  $A(x)\dotminus C(x)$ in \emph{$A\sm C$}.
Note that $|A|=|A\cap C|+|A\sm C|$.


The definitions above and those in next sections easily generalize to \emph{fractional multi-sets\/} i.e., function $A:\U\to\RR$ with non negative values.
These will have some applications in the next chapters.

As an alternative to multi-sets, we could use the following approach. Replace $\U$ with $\U'=\U\times\omega$ and $\Phi$ with $\Phi'=\{\B\times\omega\ :\ \B\in\Phi\}$. Then $\U'$ contains infinitely many copies of each element of $\U$. These copies are indistinguishable by sets in $\Phi'$. Then to every finite subset of $A\subseteq\U'$ we can associate a multi-subset of $\U$. (The correspondence is not one-to-one, but this is not relevant for the intended applications.)


If $\B\subseteq\U$ we define the \emph{frequency\/} of $\B$ over $A$

\ceq{\hfill\emph{$\Fr\big(\B/A\big)$}}{=}{\frac{\big|\B\cap A\big|}{|A|}}

Note that $\Fr(\cdot/A)$ is a probability measure on $\U$.

If $\Pr$ is a probability measure on $\U$. An \emph{$\epsilon$-approximation\/} of $\Pr$ is a sample $C$ such that  

\ceq{\hfill\displaystyle \Big|\Pr(\B) - \Fr(\B/C)\Big|}{\le}{\epsilon}

It is interesting to note that not all probability measures have an approximation. For instance let $\U=\aleph_1$ and let $\Phi$ be the $\sigma$-algebra of the countable and co-countable sets. Let $\Pr(\B)=1$ if $\B$ contains an end-segment, $\Pr(\B)=0$ otherwise. This defines a $\sigma$-additive measure which has no $\epsilon$-approximations for $\epsilon<1$. 

The \emph{$\epsilon$-approximation of a sample $A$\/} is a subsample $C\subseteq A$ such that for every definable set $\B$

\ceq{\hfill\displaystyle \Big|\Fr(\B/A) - \Fr(\B/C)\Big|}{\le}{\epsilon}

In other words, it is an $\epsilon$-approximation of the probability measure  $\Fr(\cdot/A)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrepancy}\label{epsilon_approximations}

Given $\epsilon$ we are interested in the least $n$ such that some $\epsilon\jj$approximations of size $n$ exist.
The idea is to start with an approximation of large size and reduce size at the cost of slightly enlarging $\epsilon$.
We now introduce a powerful technique to achieve this.

In general $\U$ may not be among the definable sets. As it is often convenient to include it, we write \emph{$\Phi'$\/} for $\Phi\cup\{\U\}$.

Let $C\subseteq A$ and $\B\in\Phi'$. We call the quantity 

\ceq{\hfill\emph{$\Delta_{A,C,\B}$}}{=}{|C\cap\B|-|(A{\sm} C)\cap\B|}

the \emph{discrepancy\/} of $C$ in $\B$. 
The \emph{discrepancy\/} of $C$ is

\ceq{\hfill\emph{$\Delta_{A,C}$}}{=}{\max_{\B\in\Phi'}\big|\Delta_{A,C,\B}\big|.}

Finally, the \emph{discrepancy\/} of $A$ is 

\ceq{\hfill \emph{$\Delta_A$}}{=}{\min_{C\subseteq A}\Delta_{A,C}.}

We will use the same notation but with a lowercase \emph{$\delta$\/} for the \emph{relative discrepancy}. This is obtained obtained dividing the discrepancy by the size of $A$.

The next lemma is intuitive, if an $\epsilon$-approximation has small discrepancy then we can halve its size at a small cost.

\begin{lemma}\label{lem_aprossimazionediapprossimazione}
Let $A$ be a sample of size $n$.
Let $C\subseteq A$ have relative discrepancy $\delta_{A,C}$.
Then either $C$ or $A\sm C$ is an $\epsilon$-approximation of size $\le n/2$ for $\epsilon=2\delta_{A,C}$.
\end{lemma}

\begin{proof}
%Assume for clarity that $A$ is integral, so infimum and supremum in the definition of $\delta_A$ can be replaced by minimum and maximum.
%It is straightforward to generalize the argument to fractional multi-sets (it is not needed for the application below).

Define $n^+{=}\,|C|$ and  $n^-{=}\,|A{\sm}C|$.
We may assume that $n^+\le n/2$, otherwise swap $C$ and $A\sm C$.
Then $\delta_{A,C,\U}=(n^+-n^-)/n<0$. Now, let $\B\in\Phi'$ be arbitrary

\ceq{\ssf{1.}\hfill\frac{|A\cap\B|}{n}}{=}{\frac{|C\cap\B|\, +\, |(A{\sm}C)\cap\B|}{n}}

\ceq{\hfill(*)}{=}{\frac{2|C\cap\B|}{n}\, -\, \delta_{A,C,\B}}

\ceq{}{\le}{\frac{|C\cap\B|}{n^+}\, +\, \delta_{A,C}}

We also have 

\ceq{\ssf{2.}\hfill(*)}{=}{\frac{|C\cap\B)|}{n^+}\big(1+\delta_{A,C,\U}\big)\,-\,\delta_{A,C,\B}}

\ceq{}{\ge}{\frac{|C\cap\B|}{n^+}\ -\ 2\delta_{A,C}}

Combining \ssf{1} and \ssf{2} we obtain\smallskip

\ceq{\hfill\big|\Fr(\B/A)\ -\ \Fr(\B/C)\big|}{\le}{\left|\frac{|\B\cap A|}{n}\ -\ \frac{|\B\cap C)|}{n^+}\right|}

\ceq{}{\le}{2\delta_{A,C}}\smallskip

as claimed by the lemma.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random colorings}\label{colorings}

We say that a tuple $a=\<a_1,\dots,a_n\>\in\U^n$ is an \emph{enumeration\/} of the sample $A$ if for all $x\in\U$

\ceq{\hfill A(x)}{=}{\big|\{i\ :\ a_i=x\}\big|.}

Clearly, all enumerations of $A$ have length $n=|A|$. We write \emph{$\range(a)$} for the sample enumerated by $a$.

\def\spl{{\rm smpl}}

Fix an enumeration $a=\<a_1,\dots,a_n\>$ of $A$. A tuple $c=\<c_1,\dots,c_n\>\in\{-1,+1\}^n$ is called a \emph{coloring}. To each coloring $c$ we associate a subsample of $A$, which we denote by \emph{$\spl(a,c) $}. It assigns to $x\in\U$ multiplicity $\big|\{i : a_i=x,\ c_i=+1\}\big|$.

Note that the quantity 

\ceq{\hfill\Delta_{a,c,\B}}{=}{\sum_{a_i\in\B}c_i.}

coincide with the discrepancy of $\spl(a,c)$ over $\B$.

There is probability measure on the subsamples of $A$ which is computationally convenient. We assume that $C=\spl(a,c)$ where $c=\<c_1,\dots,c_n\>\in\{\pm1\}^n$ is obtained tossing $n$ times a fair coin. 

That is, we assume an uniform distribution on the colorings and this induces a multivariate binomial distribution on the subsamples. Note that this distribution s independent of the enumeration of $A$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Useful inequalities}


\def\ceq#1#2#3{\parbox[t]{25ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

\begin{void_thm}[Proposition (Markov's inequality)]\label{Markov}
Let $X$ be a random variable with finite mean

\ceq{\hfill\Pr\Big(X\ge\epsilon\Big)}{\le}{\frac{{\rm E}(X)}{\epsilon}}
\end{void_thm}


\begin{proof}
For simplicity, we will assume that the sample space $\Omega$ is finite. Define $A=\{a\in\Omega\,:\, X(a)>\epsilon\}$.

\ceq{\hfill{\rm E}(X)}{=}{\sum_{a\in\Omega} \Pr(a)\, X(a)}

\ceq{}{=}{\sum_{a\in A} \Pr(a)\, X(a) + \sum_{a\notin A} \Pr(a)\, X(a)}

\ceq{}{\ge}{\sum_{a\in A} \Pr(a)\, X(a) + \epsilon\sum_{a\notin A} \Pr(a)}

\ceq{}{\ge}{\epsilon\sum_{a\notin A}X(a)\le\epsilon \Pr(a)}

\ceq{}{=}{\epsilon\Pr(X\le\epsilon)}

\end{proof}

\begin{void_thm}[Proposition (Chebychev's inequality)]\label{Chebyshev}
Let $X$ be a random variable with finite mean and finite variance.

\ceq{\hfill\Pr \Big(\big|{\rm E}(X)-X\big|\le\epsilon\Big)}{<}{\frac{{\rm Var}(X)}{\epsilon^{\scriptscriptstyle 2}}}
\end{void_thm}


\begin{proof}
For simplicity, we will assume that the sample space $\Omega$ is finite. Define $A=\{a\in\Omega\,:\, |E(X)-X(a)|>\epsilon\}$.

\ceq{\hfill{\rm Var}(X)}{=}{\sum_{a\in\Omega} \Pr(a)\, \big(X(a)-{\rm E}(X)\big)^2}

\ceq{}{=}{\sum_{a\in A}\Pr(a)\, \big(X(a)-{\rm E}(X)\big)^2 + \sum_{a\notin A} \Pr(a)\, \big(X(a)-{\rm E}(X)\big)^2}

\ceq{}{\ge}{\sum_{a\in A}\Pr(a)\, \big(X(a)-{\rm E}(X)\big)^2 + \epsilon^2\sum_{a\notin A} \Pr(a)}

\ceq{}{\ge}{\epsilon^2\sum_{a\notin A} \Pr(a)}

\ceq{}{=}{\epsilon^2 \Pr \Big(\big|{\rm E}(X)-X\big|\le\epsilon\Big)}
\end{proof}

\begin{void_thm}[Proposition (Weak Law of Large Numbers)]\label{wlln}
Let $\Pr$ be a probability measure on $\U$.
Then, for every event $\B\subseteq\U$ and every $n,\epsilon>0$

\ceq{\hfill\frac{1}{4n\epsilon^{\scriptscriptstyle 2}}}{\ge}{\displaystyle\Pr \Big(c\in\U^n\ :\ \Big|\Pr(\B) - \Fr(\B/c) \Big|\ge\epsilon\Big)}
\end{void_thm}


\begin{proof}
Note that $X=X(c)=\big|\range(c)\cap\B\big|$ is a binomial random variable with success probability $p=\mu\B$. 
Hence ${\rm E}(X)=p$ and  ${\rm Var}(X)=p(1-p)/n$.
Apply Chebyshev's inequality.
\end{proof}




\begin{lemma}[(Chernoff's bound, special case)]\label{Chernoff}
For $i=1,\dots,n$ let $X_i$ be independent identically distributed random variables such that\/ $\Pr(X_i=\pm1)=1/2$.
Then for every $\epsilon>0$

\ceq{\hfill \Pr\big(M\ge\epsilon\big)}{\le}{\exp(-\frac{\,n\epsilon^2}{2})}\hfill where $\displaystyle M=\sum^n_{i=1}X_i$
\end{lemma}
\begin{proof}
Let $t>0$ be arbitrary.
Then

\ceq{\sharp\hfill \Pr(M\ge\epsilon)}{=}{ \Pr\big(e^{tM}\ge e^{t\epsilon}\big)}

\ceq{~}{\le}{e^{-t\epsilon}\,{\rm E}\big(e^{tM}\big)}

In fact, the equality follows because the exponential is an increasing function and the inequality is Markov's inequality, which says that $\Pr(X\ge a)\le a^{-1}{\rm E}(X)$ for every $a$ and is immediate to verify.
Now observe that

\ceq{\hfill {\rm E}\big(e^{tX_i}\big)}{=}{\frac12e^t\ +\ \frac12e^{-t}}

\ceq{~}{=}{\frac12\sum^\infty_{i=0}\frac{t^i}{i!}\ +\ \frac12\sum^\infty_{i=0}\frac{(-t)^{i}}{i!}}

\ceq{~}{=}{\sum^\infty_{i=0}\frac{t^{2i}}{(2i)!}}

\ceq{~}{\le}{\sum^\infty_{i=0}\frac{(t^2/2)^i}{i!}}

\ceq{~}{=}{e^{t^2/2}}

From this, by independence we have 

\ceq{\hfill {\rm E}\big(e^{tM}\big)}{=}{\prod^n_{i=1}e^{t\,X_i}}$\medrel{=}e^{n\,t^2/2}$

Substituting in $\sharp$ gives $\Pr(M\ge\epsilon)\le e^{n\,t^2/2-t\epsilon}$.
Finally Chernoff's inequality is obtained substituting $\epsilon/n$ for $t$.
\end{proof}

\begin{proposition}\label{prop_disequazione}
For every $y$ and every $x>1$

\ceq{\hfill \frac{x}{\ln x}\le y}{\IMP}{x\le 2\, y\,\ln y}
\end{proposition}
\begin{proof}
It suffices to prove that for  every $x>1$

\ceq{\hfill \frac{x}{\ln x}\le y}{\IMP}{\ln x\le 2\, \ln y}

As the latter inequality is equivalent to $\sqrt{x}\le y$, it suffices to verify that $\sqrt{x}\le\displaystyle\frac{x}{\ln x}$ for all $x>1$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A uniform law of large numbers}

\def\ceq#1#2#3{\parbox[t]{35ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

We want to estimate the minimal size of an $\epsilon$-approximation of $A$.
We want a bound that depends solely on $\epsilon$, not on the size of $A$.
It is also important to note that the definition of approximation includes a requirement of uniformity: the same subsample works for all definable sets.

If we allow $C\subseteq A$ to depend on $\B$, the existence of a subsample that satisfies the inequality 

\ceq{\hfill\displaystyle \Big|\Fr(\B/A) - \Fr(\B/C)\Big|}{\le}{\epsilon}

follows easily from Chebychev's inequality and its size only depends on $\epsilon$. This is essentially the weak law of large numbers for the probability measure $\Fr(\cdot/A)$.

\def\ceq#1#2#3{\parbox[t]{25ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

\begin{proposition}[(Weak law of large numbers)]
For every sample $A$, every $\B\subseteq\U$ and every $\epsilon>0$ there is a $C$ of size $n=1/4\,\epsilon^2$ such that $\big|\Fr(\B/A) - \Fr(\B/C)\big|\le\epsilon$.
\end{proposition}
% 
% \begin{proof}
% Apply the weak law of large numbers, Proposition~\ref{wlln},  with $\mu(\B) = \Fr(A,\B)$. If $n\le1/4\,\epsilon^2$ then
% 
% \ceq{\hfill0}{<}{\displaystyle\mu^n \Big(c\in\U^n\ :\ \Big|\mu(\B) - \Fr(B/c)\big| \Big|\le\epsilon\Big)}
% 
% In particular, set above in non empty. The required subsample is $C=c^\circ\cap A$.
% \end{proof}

 Note that to avoid technicalities we do not insist in requiring $C\subseteq A$.

\begin{proof}
Interpret $\Fr(\cdot/A)$ as a probability measure on $\supp A$. Fix $\B$ and set $p=\Fr(\B/A)$. Consider the Bernoulli random variables $X_i$ that on input $c\in(\supp A)^n$ output

\ceq{\hfill X_i(c)}{=}{\left\{
\begin{array}{ll}
1&{\rm if}\ c_i\in\B\\
0&{\rm if}\ c_i\notin\B\\
\end{array}\right.}

Define also $\displaystyle\bar X=\frac1n\sum_iX_i$. Then ${\rm E}(\bar X)=p$ and $\displaystyle{\rm Var}(\bar X)=\frac{p(1-p)}{n}\le\frac1{4n}$. 

If we set $C=\range(c)$, we obtain 

\ceq{\hfill \big|{\rm E}(\bar X)-\bar X(c) \big|}{=}{\big|\Fr(\B/A) - \Fr(\B/C)\big|}

So it suffices that $\big|{\rm E}(\bar X) - \bar X\big|\le\epsilon$ has positive probability for some large enough $n$. By Chebychev's inequality

\ceq{\hfill 1-\frac{1}{4n\epsilon^{\scriptscriptstyle 2}}}{<}{\Pr\Big(\big|{\rm E}(\bar X) - \bar X\big|\le\epsilon\Big)}

% By the Chernoff bound
% 
% \ceq{\hfill 1-2\,e^{-2n\epsilon^2}}{<}{\Pr\Big(\big|{\rm E}(\bar X) - \bar X\big|\le\epsilon\Big)}

Hence the probability above is positive for $\displaystyle\frac1{4\epsilon^{\scriptscriptstyle 2}}\le n$.
\end{proof}

The main result of this chapter is the following theorem of  Vapnik and Chervonenkis which we proof in the next section.

\begin{theorem}[(Uniform law of large numbers)]\label{thm_epsilon_approx}
Assume $(\U,\Phi)$ has \vc-density $d$. Then every sample $A$ has an $\epsilon$-approximation of size 

\ceq{\hfill n}{=}{c\,\frac{d}{\epsilon^{\scriptscriptstyle 2}}\ln\frac1\epsilon,}

where $c$ is an absolute constant.
\end{theorem}



The lemma above tells that $\epsilon$-approximations with small discrepancy are useful, but as yet we have no clue as to finding one.
We are going to prove that when the number of definable subsets of $A$ is relatively small, then the discrepancy of $A$ is not too large.
We use a probabilistic argument to prove this bound (when you don't have a clue how to do something, you might as well do it randomly).


\begin{lemma}\label{lem_discrepanzarandom} 
Let $A$ be a sample of size $\le n$. Assume the support of $A$ has $\le m$  definable subsets. Then $\Delta_A\ \le\ \sqrt{2n\ln(m+2)\;}$.
\end{lemma}

\begin{proof}
\def\ceq#1#2#3{\parbox[t]{40ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}
Let $\epsilon=\sqrt{2n\ln(m+2)}$. Then we need to prove that $\Delta_A\le\epsilon$ which is equivalent to

\ceq{\hfill \E C\subseteq A\ \A\B\in\Phi'\ \Delta_{A,C,\B}}{\le}{\epsilon}

Let $\Pr(\cdot)$ denote the probability measure on the subsamples of $A$ discussed in Section~\ref{colorings} (though, clearly, any other probability measure would do for the implication)

\ceq{\rm}{\Uparrow}{}

\ceq{\hfill \Pr\big(\A\B\in\Phi'\ \ \Delta_{A,C,\B}\le \epsilon\big)}{>}{0.}

As $\Phi'$ has at most $m+1$ elements

\ceq{}{\Uparrow}{}

\ceq{\hfill\A\B\in\Phi'\ \Pr\big(\Delta_{A,C,\B}\ge\epsilon\big)}{\le}{\frac1{m+2}.}

Fix an enumeration $a$ of $A$. Suppose $c=\<c_1,\dots,c_n\>\in\{\pm1\}^n$ is obtained tossing $n$ times a fair coin. Consider the Bernoulli random variables $X_i$ that on input $c$ output $c_i$. For a given $\B$ define $\displaystyle M_\B=\sum_{a_i\in\B} X_i$. Then $\Delta_{A,C,\B}= \Delta_{a,c,\B}=M_\B(c)$. Therefore

\ceq{\hfill\Pr\big(\Delta_{A,C,\B}\ge\epsilon\big)}{\le}{\frac1{m+2}.}

\ceq{}{\Updownarrow}{} 

\ceq{\hfill\Pr\big(M_\B\ge\epsilon\big)}{\le}{\frac1{m+2}.}

\ceq{\rm By\ the\ Chernoff's\ bound}{\Uparrow}{} 

\ceq{\hfill\exp(-\frac{\epsilon^2}{2|\B|})}{\le}{\frac1{m+2}.}

\ceq{}{\Updownarrow}{} 

\ceq{\hfill2|\B|\ln(m+2)}{\le}{\epsilon^2}

As $|\B|\le n$, the latter is clearly true.
\end{proof}

\begin{void_thm}[Proof of Proposition \ref{thm_epsilon_approx}]\rm
Set $A_0=A$ and $\epsilon_0=0$. We construct a decreasing chain $A_i$ of $\epsilon_i$-approximations.
We denote by $n_i$ and $\delta_i$ the cardinality, respectively the discrepancy, of $A_i$.
By lemma~\ref{lem_aprossimazionediapprossimazione}, we can require that $\epsilon_{i+1}=\epsilon_i+2\delta_i$ and $n_{i+1}\le n_i/2$.
Then

\ceq{\hfill\epsilon_{h}}{=}{2\sum^h_{i=1}\delta_i}

Let $h$ be the largest such that $\epsilon_h\le\epsilon$. The theorem claims that $\displaystyle 2^{-h}n_0\le c\,(d/\epsilon^2)\log(1/\epsilon)$, independently of $n_0$. 


\ceq{\hfill2\sum^h_{i=1}\delta_i}{\le}{2\sum^h_{i=1}\sqrt{(2/n_i)\ln(n_i^d+2)}}

and, as $n_i=2^{-i}n_0$, the sum above is proportional to its largest term,

\ceq{}{\le}{c\sqrt{(d/n_h)\ln(n_h)}}

Hence a sufficient condition for $\epsilon_h<\epsilon$ is 

\ceq{\hfill c\sqrt{(d/n_h)\ln(n_h)}}{\le}{\epsilon}

which is equivalent to

\ceq{\hfill \frac{c^2 d}{\epsilon^2}}{\le}{\frac{n_h}{\ln n_h}}

Let $h$ be the maximal (or, $n_h$ the minimal) that satisfies this inequality. Hence


\ceq{\hfill \frac{c^2 d}{\epsilon^2}}{>}{\frac{n_{h+1}}{\ln n_{h+1}}}

By Proposition~\ref{prop_disequazione}

\ceq{\hfill n_{h+1}}{\le}{ 2\frac{c^2 d}{\epsilon^2}\ln\frac{c^2 d}{\epsilon^2}}

hence 

\ceq{\hfill n_h}{\le}{ 4\frac{c^2 d}{\epsilon^2}\ln\frac{c^2 d}{\epsilon^2}}


which proves the theorem.
\end{void_thm}


 
\end{document}
